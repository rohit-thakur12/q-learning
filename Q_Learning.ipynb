{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0-teA8Ioyrj-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer():\n",
        "  def __init__(self, max_size, input_dim):\n",
        "    self.mem_size = max_size\n",
        "    self.mem_counter = 0\n",
        "\n",
        "    self.state_memory = np.zeros((self.mem_size, *input_dim), dtype=np.float32)\n",
        "    self.new_state_memory = np.zeros((self.mem_size, *input_dim), dtype=np.float32)\n",
        "    self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
        "    self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
        "    self.terminal_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
        "\n",
        "  def store_transition(self, state, action, reward, state_, done):\n",
        "    index = self.mem_counter % self.mem_size\n",
        "    self.state_memory[index] = state\n",
        "    self.new_state_memory[index] = state_\n",
        "    self.reward_memory[index] = reward\n",
        "    self.action_memory[index] = action\n",
        "    self.terminal_memory[index] = 1 - int(done)\n",
        "    self.mem_counter += 1\n",
        "\n",
        "  def sample_buffer(self, batch_size):\n",
        "    max_mem = min(self.mem_counter, self.mem_size)\n",
        "    batch = np.random.choice(max_mem, batch_size, replace=False)\n",
        "\n",
        "    states = self.state_memory[batch]\n",
        "    states_ = self.new_state_memory[batch]\n",
        "    reward = self.reward_memory[batch]\n",
        "    action = self.action_memory[batch]\n",
        "    terminal = self.terminal_memory[batch]\n",
        "\n",
        "    return states, action, reward, states_, terminal"
      ],
      "metadata": {
        "id": "cIKzQSKv0nqs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dqn(lr, n_actions, input_dim, fc1_dims, fc2_dims):\n",
        "  model = keras.Sequential([\n",
        "                            keras.layers.Dense(fc1_dims, activation=\"relu\"),\n",
        "                            keras.layers.Dense(fc2_dims, activation='relu'),\n",
        "                            keras.layers.Dense(n_actions, activation=None)])\n",
        "  \n",
        "  opt = Adam(learning_rate=lr)\n",
        "  model.compile(optimizer=opt, loss='mean_squared_error')\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "uGUsXzj34Ajo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "  def __init__(self, lr, gamma, n_actions, epsilon, batch_size, \n",
        "               input_dims, epsilon_dec=1e-3, epsilon_end=0.01, \n",
        "               mem_size=1000000, fname='dqn_model.h5'):\n",
        "    self.action = [i for i in range(n_actions)]\n",
        "    self.gamma = gamma\n",
        "    self.epsilon = epsilon\n",
        "    self.eps_min = epsilon_end\n",
        "    self.batch_size = batch_size\n",
        "    self.model_file = fname\n",
        "    self.memory = ReplayBuffer(mem_size, input_dims)\n",
        "    self.q_eval = build_dqn(lr, n_actions, input_dims, 256, 256)\n",
        "\n",
        "  def store_transition(self, state, action, reward, new_state, done):\n",
        "    self.memory.store_transition(state, action, reward, new_state, done)\n",
        "\n",
        "  def choose_action(self, observation):\n",
        "    if np.random.random() < self.epsilon:\n",
        "      action = np.random.choice(self.action)\n",
        "    else:\n",
        "      state = np.array([observation])\n",
        "      actions = self.q_eval.predict(state)\n",
        "      action = np.argmax(actions)\n",
        "    return action \n",
        "\n",
        "  def learn(self):\n",
        "    if self.memory.mem_counter < self.batch_size:\n",
        "      return \n",
        "\n",
        "    states, actions, rewards, states_, done = self.memory.sample_buffer(self.batch_size)\n",
        "    q_eval = self.q_eval.predict(states)\n",
        "    q_next = self.q_eval.predict(states_)\n",
        "\n",
        "    q_target = np.copy(q_eval)\n",
        "    batch_index = np.argmax(self.batch_size, dtype=np.int32)\n",
        "\n",
        "    q_target[batch_index, actions] = rewards + self.gamma * np.max(q_next, axis=1) * done\n",
        "    self.q_eval.train_on_batch(states, q_target)\n",
        "    self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n",
        "\n",
        "  def save_model(self):\n",
        "    self.q_eval.save(self.model_file)\n",
        "  \n",
        "  def load_model(self):\n",
        "    self.q_eval = load_model(self.model_file)"
      ],
      "metadata": {
        "id": "U2clWbL85W9P"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade ribs[all] gym~=0.17.0 Box2D~=2.3.10 tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNGU97FuLY3q",
        "outputId": "adb33925-5861-4d2a-ce24-06aa026dbca2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ribs[all]\n",
            "  Downloading ribs-0.4.0-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym~=0.17.0 in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Collecting Box2D~=2.3.10\n",
            "  Downloading Box2D-2.3.10-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 30.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym~=0.17.0) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym~=0.17.0) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym~=0.17.0) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym~=0.17.0) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym~=0.17.0) (0.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ribs[all]) (3.0.0)\n",
            "Requirement already satisfied: toml>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from ribs[all]) (0.10.2)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ribs[all]) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ribs[all]) (1.0.2)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ribs[all]) (2.4.0)\n",
            "Requirement already satisfied: decorator>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ribs[all]) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.7/dist-packages (from ribs[all]) (0.51.2)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from ribs[all]) (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ribs[all]) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ribs[all]) (3.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ribs[all]) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ribs[all]) (1.3.2)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.51.0->ribs[all]) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.51.0->ribs[all]) (57.4.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.0->ribs[all]) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0.0->ribs[all]) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ribs[all]) (1.1.0)\n",
            "Installing collected packages: ribs, Box2D\n",
            "Successfully installed Box2D-2.3.10 ribs-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym"
      ],
      "metadata": {
        "id": "WOh-6JJCIR10"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.compat.v1.disable_eager_execution()"
      ],
      "metadata": {
        "id": "f5zGpHJHIdQo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('LunarLander-v2')\n",
        "lr = 0.001\n",
        "n_games = 500\n",
        "agent = Agent(gamma=0.99, epsilon=1.0, lr=lr, \n",
        "              input_dims=env.observation_space.shape[0], \n",
        "              n_actions=env.action_space.n, mem_size=1000000, batch_size=64, epsilon_end=0.01)\n",
        "\n",
        "scores = []\n",
        "eps_history =  []\n",
        "\n",
        "for i in range(n_games):\n",
        "  done = False\n",
        "  score = 0\n",
        "  observation = env.reset()\n",
        "  while not done:\n",
        "    action = agent.choose_action(observation)\n",
        "    observation_, reward, done, info = env.step(action)\n",
        "    score += reward\n",
        "    agent.store_transition(observation, action, reward, observation_, done)\n",
        "    observation = observation_\n",
        "    agent.learn()\n",
        "  eps_history.append(agent.epsilon)\n",
        "  scores.append(score)\n",
        "\n",
        "  avg_score = np.mean(scores[-100:])\n",
        "  print('episode: ', i, 'score %.2f' % score, \n",
        "        'average_score %.2f' % avg_score, 'epsilon %.2f' % agent.epsilon)"
      ],
      "metadata": {
        "id": "T_r2mdldIl3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vxFV5qzpLKid"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}